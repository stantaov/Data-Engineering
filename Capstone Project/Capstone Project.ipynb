{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The purpose of this project is an attempt to combine technologies learned throughout the Udacity Data Engineering program. The project encompasses four datasets. The main dataset includes data on immigration to the United States, and supplementary datasets include data on airport codes, U.S. city demographics, and temperature data. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "This a capstone project and it's a part of the Udacity Data Engineering Nanodegree program. This project mimics a real-life situation when you need to analyze, clean, save the data into a columnar file format and load the data to a data lake on S3 using Spark. Create an Airflow pipeline to load the data to the Redshift database for further analytical purposes. You can see the process below. \n",
    "\n",
    "<img src=\"process.jpg\">\n",
    "\n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The project contains four files that were gathered and provided by Udacity.\n",
    "\n",
    "I94 Immigration Data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. There is a link to the original dataset https://travel.trade.gov/research/reports/i94/historical/2016.html. A sample dataset was along provided in the workspace (immigration_data_sample.csv). The data dictionary can be found in I94_SAS_Labels_Descriptions.SAS file.\n",
    "\n",
    "#### I94 Data Dictionary\n",
    "- cicid - float64 - ID that uniquely identify one record in the dataset\n",
    "- i94yr - float64 - 4 digit year\n",
    "- i94mon- float64 - Numeric month\n",
    "- i94cit - float64 - 3 digit code of source city for immigration (Born country)\n",
    "- i94res - float64 - 3 digit code of source country for immigration (Residence country)\n",
    "- i94port - object - Port addmitted through\n",
    "- arrdate - float64 - Arrival date in the USA\n",
    "- i94mode - float64 - Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "- i94addr - object - State of arrival\n",
    "- depdate -float64 - Departure date\n",
    "- i94bir - float64 - Age of Respondent in Years\n",
    "- i94visa - float64 - Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student)\n",
    "- count - float64 - Used for summary statistics\n",
    "- dtadfile - object - Character Date Field\n",
    "- visapost - object - Department of State where where Visa was issued\n",
    "- occup - object - Occupation that will be performed in U.S.\n",
    "- entdepa - object - Arrival Flag. Whether admitted or paroled into the US\n",
    "- entdepd - object - Departure Flag. Whether departed, lost visa, or deceased\n",
    "- entdepu - object - Update Flag. Update of visa, either apprehended, overstayed, or updated to PR\n",
    "- matflag - object - Match flag\n",
    "- biryear - float64 - 4 digit year of birth\n",
    "- dtaddto - object - Character date field to when admitted in the US\n",
    "- gender - object - Gender\n",
    "- insnum - object - INS number\n",
    "- airline - object - Airline used to arrive in U.S.\n",
    "- admnum - float64 - Admission number, should be unique and not nullable\n",
    "- fltno - object - Flight number of Airline used to arrive in U.S.\n",
    "- visatype - object - Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "\n",
    "World Temperature Data comes from Kaggle. Further details about the dataset can be found here: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data.\n",
    "\n",
    "#### World Temperature Data Dictionary\n",
    "- dt - Date in format YYYY-MM-DD\n",
    "- AverageTemperature - Average temperature of the city in a given date\n",
    "- AverageTemperatureUncertainty - Standard Deviation of the avg. temperature\n",
    "- City \n",
    "- Country \n",
    "- Latitude \n",
    "- Longitude \n",
    "\n",
    "\n",
    "The U.S. City Demographic Data comes from OpenSoft. Further details about the dataset can be found here: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/.\n",
    "\n",
    "#### Demographic Data Dictionary\n",
    "- City - Name of the city\n",
    "- State - US state of the city\n",
    "- Median Age - The median of the age of the population\n",
    "- Male Population - Number of the male population\n",
    "- Female Population - Number of the female population\n",
    "- Total Population - Number of the total population\n",
    "- Number of Veterans - Number of veterans living in the city\n",
    "- Foreign-born - Number of residents of the city that were not born in the city\n",
    "- Average Household Size - Average size of the houses in the city\n",
    "- State Code - Code of the state of the city\n",
    "- Race - Race class\n",
    "- Count - Number of individual of each race\n",
    "\n",
    "Airport Date is a simple table of airport codes and corresponding cities. The data can be found here: https://datahub.io/core/airport-codes#data.\n",
    "\n",
    "### Airport Data Dictionary\n",
    "- ident - Unique identifier\n",
    "- type - Type of the airport\n",
    "- name - Airport Name\n",
    "- elevation_ft - Altitude of the airport\n",
    "- continent - Continent\n",
    "- iso_country -ISO code of the country of the airport\n",
    "- iso_region - ISO code for the region of the airport\n",
    "- municipality - City where the airport is located\n",
    "- gps_code - GPS code of the airport\n",
    "- iata_code - IATA code of the airport\n",
    "- local_code - Local code of the airport\n",
    "- coordinates - GPS coordinates of the airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import datediff, to_date, date_format\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read immigration_data_sample file to preview data\n",
    "df = pd.read_csv(\"immigration_data_sample.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 29 columns):\n",
      "Unnamed: 0    1000 non-null int64\n",
      "cicid         1000 non-null float64\n",
      "i94yr         1000 non-null float64\n",
      "i94mon        1000 non-null float64\n",
      "i94cit        1000 non-null float64\n",
      "i94res        1000 non-null float64\n",
      "i94port       1000 non-null object\n",
      "arrdate       1000 non-null float64\n",
      "i94mode       1000 non-null float64\n",
      "i94addr       941 non-null object\n",
      "depdate       951 non-null float64\n",
      "i94bir        1000 non-null float64\n",
      "i94visa       1000 non-null float64\n",
      "count         1000 non-null float64\n",
      "dtadfile      1000 non-null int64\n",
      "visapost      382 non-null object\n",
      "occup         4 non-null object\n",
      "entdepa       1000 non-null object\n",
      "entdepd       954 non-null object\n",
      "entdepu       0 non-null float64\n",
      "matflag       954 non-null object\n",
      "biryear       1000 non-null float64\n",
      "dtaddto       1000 non-null object\n",
      "gender        859 non-null object\n",
      "insnum        35 non-null float64\n",
      "airline       967 non-null object\n",
      "admnum        1000 non-null float64\n",
      "fltno         992 non-null object\n",
      "visatype      1000 non-null object\n",
      "dtypes: float64(15), int64(2), object(12)\n",
      "memory usage: 226.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>biryear</th>\n",
       "      <th>insnum</th>\n",
       "      <th>admnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>951.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.542097e+06</td>\n",
       "      <td>3.040461e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>302.928000</td>\n",
       "      <td>298.26200</td>\n",
       "      <td>20559.680000</td>\n",
       "      <td>1.078000</td>\n",
       "      <td>20575.037855</td>\n",
       "      <td>42.382000</td>\n",
       "      <td>1.859000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.016042e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1973.618000</td>\n",
       "      <td>3826.857143</td>\n",
       "      <td>6.937237e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.152879e+05</td>\n",
       "      <td>1.799818e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.485285</td>\n",
       "      <td>202.12039</td>\n",
       "      <td>8.995027</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>24.211234</td>\n",
       "      <td>17.903424</td>\n",
       "      <td>0.386353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.951657e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.903424</td>\n",
       "      <td>221.742583</td>\n",
       "      <td>2.338134e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.092500e+04</td>\n",
       "      <td>1.320800e+04</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>103.00000</td>\n",
       "      <td>20545.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20547.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.016040e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1923.000000</td>\n",
       "      <td>3468.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.214422e+05</td>\n",
       "      <td>1.412170e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>131.00000</td>\n",
       "      <td>20552.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20561.000000</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.016041e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1961.000000</td>\n",
       "      <td>3668.000000</td>\n",
       "      <td>5.599301e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.494568e+06</td>\n",
       "      <td>2.941176e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>213.00000</td>\n",
       "      <td>20560.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20570.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.016042e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1974.000000</td>\n",
       "      <td>3887.000000</td>\n",
       "      <td>5.931477e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.360901e+06</td>\n",
       "      <td>4.694151e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.00000</td>\n",
       "      <td>20567.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20580.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.016042e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1985.250000</td>\n",
       "      <td>3943.000000</td>\n",
       "      <td>9.343623e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.095749e+06</td>\n",
       "      <td>6.061994e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>746.000000</td>\n",
       "      <td>696.00000</td>\n",
       "      <td>20574.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>20715.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.016080e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>4686.000000</td>\n",
       "      <td>9.502151e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0         cicid   i94yr  i94mon       i94cit      i94res  \\\n",
       "count  1.000000e+03  1.000000e+03  1000.0  1000.0  1000.000000  1000.00000   \n",
       "mean   1.542097e+06  3.040461e+06  2016.0     4.0   302.928000   298.26200   \n",
       "std    9.152879e+05  1.799818e+06     0.0     0.0   206.485285   202.12039   \n",
       "min    1.092500e+04  1.320800e+04  2016.0     4.0   103.000000   103.00000   \n",
       "25%    7.214422e+05  1.412170e+06  2016.0     4.0   135.000000   131.00000   \n",
       "50%    1.494568e+06  2.941176e+06  2016.0     4.0   213.000000   213.00000   \n",
       "75%    2.360901e+06  4.694151e+06  2016.0     4.0   438.000000   438.00000   \n",
       "max    3.095749e+06  6.061994e+06  2016.0     4.0   746.000000   696.00000   \n",
       "\n",
       "            arrdate      i94mode       depdate       i94bir      i94visa  \\\n",
       "count   1000.000000  1000.000000    951.000000  1000.000000  1000.000000   \n",
       "mean   20559.680000     1.078000  20575.037855    42.382000     1.859000   \n",
       "std        8.995027     0.485955     24.211234    17.903424     0.386353   \n",
       "min    20545.000000     1.000000  20547.000000     1.000000     1.000000   \n",
       "25%    20552.000000     1.000000  20561.000000    30.750000     2.000000   \n",
       "50%    20560.000000     1.000000  20570.000000    42.000000     2.000000   \n",
       "75%    20567.250000     1.000000  20580.000000    55.000000     2.000000   \n",
       "max    20574.000000     9.000000  20715.000000    93.000000     3.000000   \n",
       "\n",
       "        count      dtadfile  entdepu      biryear       insnum        admnum  \n",
       "count  1000.0  1.000000e+03      0.0  1000.000000    35.000000  1.000000e+03  \n",
       "mean      1.0  2.016042e+07      NaN  1973.618000  3826.857143  6.937237e+10  \n",
       "std       0.0  4.951657e+01      NaN    17.903424   221.742583  2.338134e+10  \n",
       "min       1.0  2.016040e+07      NaN  1923.000000  3468.000000  0.000000e+00  \n",
       "25%       1.0  2.016041e+07      NaN  1961.000000  3668.000000  5.599301e+10  \n",
       "50%       1.0  2.016042e+07      NaN  1974.000000  3887.000000  5.931477e+10  \n",
       "75%       1.0  2.016042e+07      NaN  1985.250000  3943.000000  9.343623e+10  \n",
       "max       1.0  2.016080e+07      NaN  2015.000000  4686.000000  9.502151e+10  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0.0\n",
       "cicid           0.0\n",
       "i94yr           0.0\n",
       "i94mon          0.0\n",
       "i94cit          0.0\n",
       "i94res          0.0\n",
       "i94port         0.0\n",
       "arrdate         0.0\n",
       "i94mode         0.0\n",
       "i94addr         5.9\n",
       "depdate         4.9\n",
       "i94bir          0.0\n",
       "i94visa         0.0\n",
       "count           0.0\n",
       "dtadfile        0.0\n",
       "visapost       61.8\n",
       "occup          99.6\n",
       "entdepa         0.0\n",
       "entdepd         4.6\n",
       "entdepu       100.0\n",
       "matflag         4.6\n",
       "biryear         0.0\n",
       "dtaddto         0.0\n",
       "gender         14.1\n",
       "insnum         96.5\n",
       "airline         3.3\n",
       "admnum          0.0\n",
       "fltno           0.8\n",
       "visatype        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display missing values in %\n",
    "(df.isnull().sum() / len(df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see some columns have missing values, interestingly gender, i94addr, airline, fltno have some NA values.\n",
    "airline and fltno have different proportions of missing values. occup and insnum columns have over 95% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[['cicid', 'i94addr', 'visapost']]\n",
    "size = df1.groupby('cicid')['visapost','i94addr'].size().reset_index()\n",
    "len(size[size[0] > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It seems there are no duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i94mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     i94mode\n",
       "1.0      962\n",
       "3.0       26\n",
       "2.0       10\n",
       "9.0        2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['i94mode']].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "i94mode - float64 - Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "As we are observing the sampled data we can conclude that most visitors enter the U.S. via Air with a small fraction by Sea. However, some people didn't declare how they enter the U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender\n",
       "M     471\n",
       "F     386\n",
       "X       2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['gender']].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There are slightly more male visitors than female and a few visitors have not declared gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid_count</th>\n",
       "      <th>i94bir_min</th>\n",
       "      <th>i94bir_max</th>\n",
       "      <th>visatype_unique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>[WT, B2, CP, B1, GMT, WB, F1, E2, F2, M1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>[WT, B1, B2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>26</td>\n",
       "      <td>17.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>[WT, B2, B1, F1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>[WT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cicid_count  i94bir_min  i94bir_max  \\\n",
       "i94mode                                        \n",
       "1.0              962         1.0        93.0   \n",
       "2.0               10        11.0        88.0   \n",
       "3.0               26        17.0        65.0   \n",
       "9.0                2        39.0        55.0   \n",
       "\n",
       "                                   visatype_unique  \n",
       "i94mode                                             \n",
       "1.0      [WT, B2, CP, B1, GMT, WB, F1, E2, F2, M1]  \n",
       "2.0                                   [WT, B1, B2]  \n",
       "3.0                               [WT, B2, B1, F1]  \n",
       "9.0                                           [WT]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.groupby('i94mode').agg(\n",
    "{'cicid': 'count',\n",
    " 'i94bir': [min, max], \n",
    " 'visatype': ['unique']\n",
    "})\n",
    "df2.columns = [\"_\".join(x) for x in df2.columns.ravel()]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It seems people who entered the U.S. by Air have wider spread by age with the youngest visitor = 1 year and the oldest = 93 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport = pd.read_csv(\"airport-codes_csv.csv\")\n",
    "airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "airport.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevation_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48069.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1240.789677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1602.363459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>718.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       elevation_ft\n",
       "count  48069.000000\n",
       "mean    1240.789677\n",
       "std     1602.363459\n",
       "min    -1266.000000\n",
       "25%      205.000000\n",
       "50%      718.000000\n",
       "75%     1497.000000\n",
       "max    22000.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident            0.000000\n",
       "type             0.000000\n",
       "name             0.000000\n",
       "elevation_ft    12.720835\n",
       "continent       50.329551\n",
       "iso_country      0.448479\n",
       "iso_region       0.000000\n",
       "municipality    10.305946\n",
       "gps_code        25.501589\n",
       "iata_code       83.315479\n",
       "local_code      47.914662\n",
       "coordinates      0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display missing values in %\n",
    "(airport.isnull().sum() / len(airport))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see there are some missing values in the airport dataset. It is important to notice that some values are missing in crucial columns like municipality, iata_code, local_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport1 = airport[['ident', 'name', 'municipality']]\n",
    "size = airport1.groupby('ident')['name','municipality'].size().reset_index()\n",
    "len(size[size[0] > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "It seems there are no duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '/data2/GlobalLandTemperaturesByCity.csv'\n",
    "temp_df =pd.read_csv(fname)\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The temperature data set has the oldest record dated 1743-11-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8599207</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>11.464</td>\n",
       "      <td>0.236</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599208</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>15.043</td>\n",
       "      <td>0.261</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599209</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>18.775</td>\n",
       "      <td>0.193</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599210</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>18.025</td>\n",
       "      <td>0.298</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599211</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "8599207  2013-05-01              11.464                          0.236   \n",
       "8599208  2013-06-01              15.043                          0.261   \n",
       "8599209  2013-07-01              18.775                          0.193   \n",
       "8599210  2013-08-01              18.025                          0.298   \n",
       "8599211  2013-09-01                 NaN                            NaN   \n",
       "\n",
       "           City      Country Latitude Longitude  \n",
       "8599207  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599208  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599209  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599210  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599211  Zwolle  Netherlands   52.24N     5.26E  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The newest record dated 2013-09-01, it seems like 2013 doesn't have the average temperature for all 12 months, therefore we will be using the whole 2012 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "temp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.235082e+06</td>\n",
       "      <td>8.235082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.672743e+01</td>\n",
       "      <td>1.028575e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.035344e+01</td>\n",
       "      <td>1.129733e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.270400e+01</td>\n",
       "      <td>3.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.029900e+01</td>\n",
       "      <td>3.370000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.883100e+01</td>\n",
       "      <td>5.910000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.521000e+01</td>\n",
       "      <td>1.349000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.965100e+01</td>\n",
       "      <td>1.539600e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AverageTemperature  AverageTemperatureUncertainty\n",
       "count        8.235082e+06                   8.235082e+06\n",
       "mean         1.672743e+01                   1.028575e+00\n",
       "std          1.035344e+01                   1.129733e+00\n",
       "min         -4.270400e+01                   3.400000e-02\n",
       "25%          1.029900e+01                   3.370000e-01\n",
       "50%          1.883100e+01                   5.910000e-01\n",
       "75%          2.521000e+01                   1.349000e+00\n",
       "max          3.965100e+01                   1.539600e+01"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               0.000000\n",
       "AverageTemperature               4.234458\n",
       "AverageTemperatureUncertainty    4.234458\n",
       "City                             0.000000\n",
       "Country                          0.000000\n",
       "Latitude                         0.000000\n",
       "Longitude                        0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display missing values in %\n",
    "(temp_df.isnull().sum() / len(temp_df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "There is a very small fraction of missing values in AverageTemperature and AverageTemperatureUncertainty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'us-cities-demographics.csv'\n",
    "demo_df = pd.read_csv(fname, ';')\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "demo_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2891.000000</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2878.000000</td>\n",
       "      <td>2.878000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2.891000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.494881</td>\n",
       "      <td>9.732843e+04</td>\n",
       "      <td>1.017696e+05</td>\n",
       "      <td>1.989668e+05</td>\n",
       "      <td>9367.832523</td>\n",
       "      <td>4.065360e+04</td>\n",
       "      <td>2.742543</td>\n",
       "      <td>4.896377e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.401617</td>\n",
       "      <td>2.162999e+05</td>\n",
       "      <td>2.315646e+05</td>\n",
       "      <td>4.475559e+05</td>\n",
       "      <td>13211.219924</td>\n",
       "      <td>1.557491e+05</td>\n",
       "      <td>0.433291</td>\n",
       "      <td>1.443856e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>22.900000</td>\n",
       "      <td>2.928100e+04</td>\n",
       "      <td>2.734800e+04</td>\n",
       "      <td>6.321500e+04</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>8.610000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.800000</td>\n",
       "      <td>3.928900e+04</td>\n",
       "      <td>4.122700e+04</td>\n",
       "      <td>8.042900e+04</td>\n",
       "      <td>3739.000000</td>\n",
       "      <td>9.224000e+03</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>3.435000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.300000</td>\n",
       "      <td>5.234100e+04</td>\n",
       "      <td>5.380900e+04</td>\n",
       "      <td>1.067820e+05</td>\n",
       "      <td>5397.000000</td>\n",
       "      <td>1.882200e+04</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>1.378000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.664175e+04</td>\n",
       "      <td>8.960400e+04</td>\n",
       "      <td>1.752320e+05</td>\n",
       "      <td>9368.000000</td>\n",
       "      <td>3.397175e+04</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>5.444700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.500000</td>\n",
       "      <td>4.081698e+06</td>\n",
       "      <td>4.468707e+06</td>\n",
       "      <td>8.550405e+06</td>\n",
       "      <td>156961.000000</td>\n",
       "      <td>3.212500e+06</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>3.835726e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Median Age  Male Population  Female Population  Total Population  \\\n",
       "count  2891.000000     2.888000e+03       2.888000e+03      2.891000e+03   \n",
       "mean     35.494881     9.732843e+04       1.017696e+05      1.989668e+05   \n",
       "std       4.401617     2.162999e+05       2.315646e+05      4.475559e+05   \n",
       "min      22.900000     2.928100e+04       2.734800e+04      6.321500e+04   \n",
       "25%      32.800000     3.928900e+04       4.122700e+04      8.042900e+04   \n",
       "50%      35.300000     5.234100e+04       5.380900e+04      1.067820e+05   \n",
       "75%      38.000000     8.664175e+04       8.960400e+04      1.752320e+05   \n",
       "max      70.500000     4.081698e+06       4.468707e+06      8.550405e+06   \n",
       "\n",
       "       Number of Veterans  Foreign-born  Average Household Size         Count  \n",
       "count         2878.000000  2.878000e+03             2875.000000  2.891000e+03  \n",
       "mean          9367.832523  4.065360e+04                2.742543  4.896377e+04  \n",
       "std          13211.219924  1.557491e+05                0.433291  1.443856e+05  \n",
       "min            416.000000  8.610000e+02                2.000000  9.800000e+01  \n",
       "25%           3739.000000  9.224000e+03                2.430000  3.435000e+03  \n",
       "50%           5397.000000  1.882200e+04                2.650000  1.378000e+04  \n",
       "75%           9368.000000  3.397175e+04                2.950000  5.444700e+04  \n",
       "max         156961.000000  3.212500e+06                4.980000  3.835726e+06  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                      0.000000\n",
       "State                     0.000000\n",
       "Median Age                0.000000\n",
       "Male Population           0.103770\n",
       "Female Population         0.103770\n",
       "Total Population          0.000000\n",
       "Number of Veterans        0.449671\n",
       "Foreign-born              0.449671\n",
       "Average Household Size    0.553442\n",
       "State Code                0.000000\n",
       "Race                      0.000000\n",
       "Count                     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display missing values in %\n",
    "(demo_df.isnull().sum() / len(demo_df))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The dataset looks complete only a few columns are missing a small fraction of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#i94_small = i94_small.join(df, df.code == i94_small.port, how = 'left')\n",
    "#i94_small = i94_small.join(temperatures, (temperatures.city == poop.city) & (temperatures.month == poop.month) & (temperatures.day == poop.day), how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Star schema will be used for this project. The schemas will contain a fact table with multiple dimensional tables. Below you can see current tables.\n",
    "\n",
    "<img src=\"current_tables.jpg\">\n",
    "\n",
    "And the following Data Model is considered.\n",
    "\n",
    "### Data Model\n",
    "\n",
    "<img src=\"data_model.jpg\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The data pipeline consists of 15 tables and allows to provide analytical information for the immigration department based on multiple parameters, like temperature on arrival city, the volume of visitors by the month of the year and many other insights that can help with planing the workload. The pipeline contains the following steps. \n",
    "\n",
    "- Read I94 SAS files to spark data frame, rename columns, remove \n",
    "- Rename columns to a more readable format\n",
    "- Remove null values from dtadfile since we need to use it as a primary key\n",
    "- Convert dtaddto and date_created to to_date format\n",
    "- Create a new column stayed_days to define the number of days each visitor stayed\n",
    "- Create a day column that can be used along month and city to map the temperature table \n",
    "- Create the airport table where iata_code is not null and iso_country is US\n",
    "- For airport table convert iso_region into new column called state\n",
    "- Create the temperature table where Country is United States and year is 2012\n",
    "- Create the demographic table and rename columns to easy-read names.\n",
    "- Define two functions to create five mapping tables for I94 data. \n",
    "- Save all tables in parquet format and load them to data lake S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read AWS keys to access S3 storage\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS CREDS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS CREDS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.1.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "    .config(\"spark.driver.memory\", \"15g\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")    \n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "sc._jsc.hadoopConfiguration().set(\"spark.speculation\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load one month of i94 (immigration data) in sas format and drop all duplicates\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat').drop_duplicates()\n",
    "# drop all null values in dtadfile column\n",
    "df_spark = df_spark.where(col(\"dtadfile\").isNotNull())\n",
    "# conver dtaddto to proper format\n",
    "df_spark = df_spark.withColumn(\"allowed_stay_till\",  to_date(\"dtaddto\",\"MMddyyyy\"))\n",
    "# conver dtadfile to proper format\n",
    "df_spark = df_spark.withColumn('date_created', to_date(\"dtadfile\", 'yyyyMMdd'))\n",
    "# find difference between dtaddto and dtadfile\n",
    "df_spark = df_spark.withColumn(\"allowed_stay_days\", datediff('allowed_stay_till', 'date_created'))\n",
    "# create stayed days for each record\n",
    "df_spark = df_spark.withColumn('stayed_days', ( df_spark['depdate'] - df_spark['arrdate']))\n",
    "# create day column\n",
    "df_spark = df_spark.withColumn('day', dayofmonth(\"date_created\"))\n",
    "# create a new view and name it as i94\n",
    "df_spark.createOrReplaceTempView('i94')\n",
    "\n",
    "query = \"\"\"SELECT cicid, \n",
    "                date_created, \n",
    "                i94yr AS year, \n",
    "                i94mon AS month,\n",
    "                day,\n",
    "                i94cit AS citizenship,\n",
    "                i94res AS resident,\n",
    "                i94bir AS age,\n",
    "                biryear AS birth_year,\n",
    "                gender,\n",
    "                occup AS occupation,\n",
    "                allowed_stay_till,\n",
    "                allowed_stay_days,\n",
    "                stayed_days,\n",
    "                i94visa AS visa_class,\n",
    "                visatype AS visa_type,\n",
    "                i94port AS port,\n",
    "                i94mode AS mode,\n",
    "                i94addr AS arraval_state,\n",
    "                visapost AS visa_issued_by,\n",
    "                entdepa AS arrival_flag,\n",
    "                entdepd AS depart_flag,\n",
    "                entdepu AS update_flag,\n",
    "                matflag AS match_flag,\n",
    "                airline,\n",
    "                admnum AS admission_number,\n",
    "                fltno AS flight_number\n",
    "           FROM i94\n",
    "       \"\"\"\n",
    "i94 = spark.sql(query)\n",
    "\n",
    "# since schema was infered by default we need to change soma data types\n",
    "i94 = i94.withColumn(\"cicid\", i94[\"cicid\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"year\", i94[\"year\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"month\", i94[\"month\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"citizenship\", i94[\"citizenship\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"resident\", i94[\"resident\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"age\", i94[\"age\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"birth_year\", i94[\"birth_year\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"stayed_days\", i94[\"stayed_days\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"visa_class\", i94[\"visa_class\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"mode\", i94[\"mode\"].cast(IntegerType()))\n",
    "i94 = i94.withColumn(\"admission_number\", i94[\"admission_number\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save i94 data to parquet format on S3\n",
    "output_data = \"s3a://udacity-data-engineering-stan/data/\"\n",
    "i94.write.parquet(output_data + 'i94/', 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read parquet file from S3\n",
    "df_spark2 = spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/i94/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+-------+-------------+-----+---+\n",
      "|    id|avg_temp|sd_temp|   city|      country|month|day|\n",
      "+------+--------+-------+-------+-------------+-----+---+\n",
      "| 49859|   7.996|  0.204|abilene|United States|    1|  1|\n",
      "| 49860|   8.434|  0.252|abilene|United States|    2|  1|\n",
      "| 49861|  15.628|  0.173|abilene|United States|    3|  1|\n",
      "| 49862|  21.069|  0.388|abilene|United States|    4|  1|\n",
      "| 49863|  24.698|  0.323|abilene|United States|    5|  1|\n",
      "| 49864|  28.217|  0.126|abilene|United States|    6|  1|\n",
      "| 49865|  29.581|  0.288|abilene|United States|    7|  1|\n",
      "| 49866|  29.104|  0.322|abilene|United States|    8|  1|\n",
      "| 49867|  24.333|  0.313|abilene|United States|    9|  1|\n",
      "| 49868|  16.702|  0.264|abilene|United States|   10|  1|\n",
      "| 49869|  13.892|  0.286|abilene|United States|   11|  1|\n",
      "| 49870|   7.951|  0.286|abilene|United States|   12|  1|\n",
      "|140284|  -0.344|   0.41|  akron|United States|    1|  1|\n",
      "|140285|   1.527|  0.319|  akron|United States|    2|  1|\n",
      "|140286|  10.109|  0.442|  akron|United States|    3|  1|\n",
      "|140287|   9.195|  0.412|  akron|United States|    4|  1|\n",
      "|140288|  18.921|  0.322|  akron|United States|    5|  1|\n",
      "|140289|  21.108|  0.298|  akron|United States|    6|  1|\n",
      "|140290|  24.966|  0.401|  akron|United States|    7|  1|\n",
      "|140291|   21.94|  0.328|  akron|United States|    8|  1|\n",
      "+------+--------+-------+-------+-------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load temperature data\n",
    "fname = '/data2/GlobalLandTemperaturesByCity.csv'\n",
    "temp_df = spark.read.csv(fname, header=True)\n",
    "# create year and month columns\n",
    "temp_df = temp_df.withColumn('month', month(\"dt\"))\\\n",
    "        .withColumn('year', year('dt'))\\\n",
    "        .withColumn('day', dayofmonth('dt'))\\\n",
    "        .withColumn('id', monotonically_increasing_id())\n",
    "# create a new view \n",
    "temp_df.createOrReplaceTempView('temperatures')\n",
    "# select only data for 2012 and United States\n",
    "query = \"\"\"SELECT id,\n",
    "                  AverageTemperature AS avg_temp,\n",
    "                  AverageTemperatureUncertainty AS sd_temp,\n",
    "                  LOWER(City) AS city,\n",
    "                  Country AS country,\n",
    "                  month,\n",
    "                  day\n",
    "             FROM temperatures\n",
    "             WHERE Country = 'United States'\n",
    "             AND year = 2012\n",
    "        \"\"\"\n",
    "temperatures = spark.sql(query)\n",
    "\n",
    "temperatures = temperatures.withColumn(\"avg_temp\", temperatures[\"avg_temp\"].cast(FloatType()))\n",
    "temperatures = temperatures.withColumn(\"sd_temp\", temperatures[\"sd_temp\"].cast(FloatType()))\n",
    "temperatures = temperatures.withColumn(\"id\", temperatures[\"id\"].cast(IntegerType()))\n",
    "\n",
    "#save data in parquet format on S3 \n",
    "temperatures.write.parquet(output_data + 'temperatures/', 'overwrite')\n",
    "# load data from S3 \n",
    "df_temp=spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/temperatures/\")\n",
    "# display data\n",
    "temperatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+------------+-----------+--------------------+--------+------------+--------------------+-----+\n",
      "|ident|          type|                name|elevation_ft|iso_country|        municipality|gps_code|airport_code|         coordinates|state|\n",
      "+-----+--------------+--------------------+------------+-----------+--------------------+--------+------------+--------------------+-----+\n",
      "|  CZK| small_airport|Cascade Locks Sta...|         151|         US|       Cascade Locks|    KCZK|         CZK|-121.878997803, 4...|   OR|\n",
      "| KATL| large_airport|Hartsfield Jackso...|        1026|         US|             Atlanta|    KATL|         ATL| -84.428101, 33.6367|   GA|\n",
      "| KECS| small_airport|       Mondell Field|        4174|         US|           Newcastle|    KECS|         ECS|-104.318001, 43.8...|   WY|\n",
      "| PASY|medium_airport|Eareckson Air Sta...|          95|         US|              Shemya|    PASY|         SYA|174.1139984, 52.7...|   AK|\n",
      "|  SAS| small_airport|  Salton Sea Airport|         -84|         US|         Salton City|    KSAS|         SAS|-115.952003479, 3...|   CA|\n",
      "| K47N| small_airport|Central Jersey Re...|          86|         US|            Manville|     47N|         JVI|-74.5983963013000...|   NJ|\n",
      "| KACT|medium_airport|Waco Regional Air...|         516|         US|                Waco|    KACT|         ACT|-97.2304992675781...|   TX|\n",
      "| KBHB|medium_airport|Hancock County-Ba...|          83|         US|          Bar Harbor|    KBHB|         BHB|-68.3615036, 44.4...|   ME|\n",
      "| KBLM| small_airport|Monmouth Executiv...|         153|         US|  Belmar/Farmingdale|    KBLM|         BLM|-74.12490082, 40....|   NJ|\n",
      "| KICT| large_airport|Wichita Eisenhowe...|        1333|         US|             Wichita|    KICT|         ICT|-97.433098, 37.64...|   KS|\n",
      "| KMAE| small_airport|Madera Municipal ...|         255|         US|              Madera|    KMAE|         MAE|-120.111999512, 3...|   CA|\n",
      "| KMFE|medium_airport|Mc Allen Miller I...|         107|         US|            Mc Allen|    KMFE|         MFE|-98.23860168, 26....|   TX|\n",
      "| KMMI| small_airport|McMinn County Air...|         875|         US|              Athens|    KMMI|         MMI|-84.56259918, 35....|   TN|\n",
      "| KOLV| small_airport|Olive Branch Airport|         402|         US|        Olive Branch|    KOLV|         OLV|-89.7869033813000...|   MS|\n",
      "| KPAO| small_airport|Palo Alto Airport...|           4|         US|           Palo Alto|    KPAO|         PAO|-122.114997864, 3...|   CA|\n",
      "| KPKB|medium_airport|Mid Ohio Valley R...|         858|         US|         Parkersburg|    KPKB|         PKB|-81.4392013549804...|   WV|\n",
      "| KRRL| small_airport|Merrill Municipal...|        1318|         US|             Merrill|    KRRL|         RRL|-89.7128982544, 4...|   WI|\n",
      "| KSIV| small_airport|Sullivan County A...|         540|         US|          Monticello|    KSIV|         SIV|-87.448303222656,...|   IN|\n",
      "| KSOP| small_airport|Moore County Airport|         455|         US|Pinehurst/Souther...|    KSOP|         SOP|-79.3911972, 35.2...|   NC|\n",
      "| KTDW| small_airport|   Tradewind Airport|        3649|         US|            Amarillo|    KTDW|         TDW|-101.825996399, 3...|   TX|\n",
      "+-----+--------------+--------------------+------------+-----------+--------------------+--------+------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load airport data\n",
    "fname = 'airport-codes_csv.csv'\n",
    "# load airport-codes_csv.csv data\n",
    "airport_df = spark.read.csv(fname, header=True, inferSchema=True).drop_duplicates()\n",
    "# create a new view \n",
    "airport_df.createOrReplaceTempView('airports')\n",
    "# select iata_code that is not null and not nan and only in US\n",
    "query = \"\"\"SELECT ident, \n",
    "                  type, \n",
    "                  name, \n",
    "                  elevation_ft, \n",
    "                  iso_country, \n",
    "                  iso_region, \n",
    "                  municipality, \n",
    "                  gps_code, \n",
    "                  iata_code AS airport_code, \n",
    "                  coordinates\n",
    "             FROM airports \n",
    "             WHERE iata_code IS NOT NULL \n",
    "             AND NOT iata_code = 'nan'\n",
    "             AND iso_country = 'US'\n",
    "        \"\"\"\n",
    "airports = spark.sql(query)\n",
    "\n",
    "# convert iso_region column to state column\n",
    "def state(iso_region):\n",
    "    return iso_region.strip().split('-')[-1]\n",
    "udf_state = udf(lambda iso_region: state(iso_region), StringType())\n",
    "\n",
    "airports = airports.withColumn('state', udf_state('iso_region')).drop('iso_region')\n",
    "airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+------------+-----------+--------------------+--------+------------+--------------------+-----+\n",
      "|ident|          type|                name|elevation_ft|iso_country|        municipality|gps_code|airport_code|         coordinates|state|\n",
      "+-----+--------------+--------------------+------------+-----------+--------------------+--------+------------+--------------------+-----+\n",
      "|  9A8| small_airport|     Ugashik Airport|          25|         US|             Ugashik|     9A8|         UGS|-157.399002075, 5...|   AK|\n",
      "| CT88|      heliport| Rentschler Heliport|          30|         US|       East Hartford|    CT88|         EHT|   -72.6253, 41.7517|   CT|\n",
      "| K0K7| small_airport|Humboldt Municipa...|        1093|         US|            Humboldt|     0K7|         HUD|-94.2452011108, 4...|   IA|\n",
      "| KABQ| large_airport|Albuquerque Inter...|        5355|         US|         Albuquerque|    KABQ|         ABQ|-106.609001, 35.0...|   NM|\n",
      "| KCEF|medium_airport|Westover ARB/Metr...|         241|         US|Springfield/Chicopee|    KCEF|         CEF|-72.53479767, 42....|   MA|\n",
      "| KCKM| small_airport|      Fletcher Field|         173|         US|          Clarksdale|    KCKM|         CKM|-90.512298584, 34...|   MS|\n",
      "| KDVO| small_airport|Marin County Airp...|           2|         US|              Novato|    KDVO|         NOT|-122.55599975586,...|   CA|\n",
      "| KFBL| small_airport|Faribault Municip...|        1060|         US|           Faribault|    KFBL|         FBL|-93.312534, 44.32844|   MN|\n",
      "| KHNB| small_airport| Huntingburg Airport|         529|         US|         Huntingburg|    KHNB|         HNB|-86.9536972045999...|   IN|\n",
      "| KLGA| large_airport|  La Guardia Airport|          21|         US|            New York|    KLGA|         LGA|-73.87259674, 40....|   NY|\n",
      "| KLSK| small_airport|Lusk Municipal Ai...|        4964|         US|                Lusk|    KLSK|         LSK|-104.404998779, 4...|   WY|\n",
      "| KMBS| large_airport|MBS International...|         668|         US|             Saginaw|    KMBS|         MBS|-84.0795974731445...|   MI|\n",
      "| KMVL| small_airport|Morrisville Stowe...|         732|         US|         Morrisville|    KMVL|         MVL|-72.6139984131, 4...|   VT|\n",
      "| KOSH|medium_airport|Wittman Regional ...|         808|         US|             Oshkosh|    KOSH|         OSH|-88.5569992065429...|   WI|\n",
      "| KPDC| small_airport|Prairie Du Chien ...|         661|         US|    Prairie Du Chien|    KPDC|         PCD|-91.12370300293, ...|   WI|\n",
      "| KRCK| small_airport|H H Coffield Regi...|         474|         US|            Rockdale|    KRCK|         RCK|-96.9897003174, 3...|   TX|\n",
      "| KSTE| small_airport|Stevens Point Mun...|        1110|         US|       Stevens Point|    KSTE|         STE|-89.530296325684,...|   WI|\n",
      "| KWJF|medium_airport|General WM J Fox ...|        2351|         US|           Lancaster|    KWJF|         WJF|-118.2190018, 34....|   CA|\n",
      "| PFEL| small_airport|        Elim Airport|         162|         US|                Elim|    PFEL|         ELI|-162.2720032, 64....|   AK|\n",
      "|  Z13| small_airport|    Akiachak Airport|          23|         US|            Akiachak|     Z13|         KKI|-161.42199707031,...|   AK|\n",
      "+-----+--------------+--------------------+------------+-----------+--------------------+--------+------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save data in parquet format on S3\n",
    "output_data = \"s3a://udacity-data-engineering-stan/data/\"\n",
    "airports.write.mode('overwrite').parquet(output_data + \"airports_data/\")\n",
    "# load data from S3 \n",
    "df_airports=spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/airports_data/\")\n",
    "# display data\n",
    "df_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---------------+-----------------+----------+------------+------------+------------------+-----+--------------------+------+\n",
      "|            city|median_age|male_population|female_population|population|num_veterans|foreign_born|avg_household_size|state|                race| count|\n",
      "+----------------+----------+---------------+-----------------+----------+------------+------------+------------------+-----+--------------------+------+\n",
      "|   Silver Spring|      33.8|        40601.0|          41862.0|     82463|      1562.0|     30908.0|               2.6|   MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy|      41.0|        44129.0|          49500.0|     93629|      4147.0|     32935.0|              2.39|   MA|               White| 58723|\n",
      "|          Hoover|      38.5|        38040.0|          46799.0|     84839|      4819.0|      8229.0|              2.58|   AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|      34.5|        88127.0|          87105.0|    175232|      5821.0|     33878.0|              3.18|   CA|Black or African-...| 24437|\n",
      "|          Newark|      34.6|       138040.0|         143873.0|    281913|      5829.0|     86253.0|              2.73|   NJ|               White| 76402|\n",
      "|          Peoria|      33.1|        56229.0|          62432.0|    118661|      6634.0|      7517.0|               2.4|   IL|American Indian a...|  1343|\n",
      "|        Avondale|      29.1|        38712.0|          41971.0|     80683|      4815.0|      8355.0|              3.18|   AZ|Black or African-...| 11592|\n",
      "|     West Covina|      39.8|        51629.0|          56860.0|    108489|      3800.0|     37038.0|              3.56|   CA|               Asian| 32716|\n",
      "|        O'Fallon|      36.0|        41762.0|          43270.0|     85032|      5783.0|      3269.0|              2.77|   MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|      35.5|        51751.0|          58077.0|    109828|      5204.0|     16315.0|              2.65|   NC|               Asian| 11060|\n",
      "|          Folsom|      40.9|        41051.0|          35317.0|     76368|      4187.0|     13234.0|              2.62|   CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|      40.9|        41051.0|          35317.0|     76368|      4187.0|     13234.0|              2.62|   CA|American Indian a...|   998|\n",
      "|    Philadelphia|      34.1|       741270.0|         826172.0|   1567442|     61995.0|    205339.0|              2.61|   PA|               Asian|122721|\n",
      "|         Wichita|      34.6|       192354.0|         197601.0|    389955|     23978.0|     40270.0|              2.56|   KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|      34.6|       192354.0|         197601.0|    389955|     23978.0|     40270.0|              2.56|   KS|American Indian a...|  8791|\n",
      "|      Fort Myers|      37.3|        36850.0|          37165.0|     74015|      4312.0|     15365.0|              2.45|   FL|               White| 50169|\n",
      "|      Pittsburgh|      32.9|       149690.0|         154695.0|    304385|     17728.0|     28187.0|              2.13|   PA|               White|208863|\n",
      "|          Laredo|      28.8|       124305.0|         131484.0|    255789|      4921.0|     68427.0|              3.66|   TX|American Indian a...|  1253|\n",
      "|        Berkeley|      32.5|        60142.0|          60829.0|    120971|      3736.0|     25000.0|              2.35|   CA|               Asian| 27089|\n",
      "|     Santa Clara|      35.2|        63278.0|          62938.0|    126216|      4426.0|     52281.0|              2.75|   CA|               White| 55847|\n",
      "+----------------+----------+---------------+-----------------+----------+------------+------------+------------------+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load demographics data\n",
    "fname = 'us-cities-demographics.csv'\n",
    "demo_df = pd.read_csv(fname, ';')\n",
    "# create spark dataframe\n",
    "demo_df = spark.createDataFrame(demo_df)\n",
    "# create a new view\n",
    "demo_df.createOrReplaceTempView('demographics')\n",
    "query = \"\"\"SELECT city, \n",
    "                 `Median Age` AS median_age, \n",
    "                 `Male Population` AS male_population,\n",
    "                 `Female Population` AS female_population, \n",
    "                 `Total Population` AS population,\n",
    "                 `Number of Veterans` AS num_veterans, \n",
    "                 `Foreign-born` AS foreign_born, \n",
    "                 `Average Household Size` AS avg_household_size,\n",
    "                 `State Code` AS state, \n",
    "                 race, \n",
    "                 count\n",
    "            FROM demographics\"\"\"\n",
    "us_cities_demo = spark.sql(query)\n",
    "\n",
    "us_cities_demo.show()\n",
    "\n",
    "us_cities_demo = us_cities_demo.withColumn(\"male_population\", us_cities_demo[\"male_population\"].cast(IntegerType()))\n",
    "us_cities_demo = us_cities_demo.withColumn(\"female_population\", us_cities_demo[\"female_population\"].cast(IntegerType()))\n",
    "us_cities_demo = us_cities_demo.withColumn(\"num_veterans\", us_cities_demo[\"num_veterans\"].cast(IntegerType()))\n",
    "us_cities_demo = us_cities_demo.withColumn(\"foreign_born\", us_cities_demo[\"foreign_born\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+---------------+-----------------+----------+------------+------------+------------------+-----+--------------------+------+\n",
      "|            city|median_age|male_population|female_population|population|num_veterans|foreign_born|avg_household_size|state|                race| count|\n",
      "+----------------+----------+---------------+-----------------+----------+------------+------------+------------------+-----+--------------------+------+\n",
      "|   Silver Spring|      33.8|          40601|            41862|     82463|        1562|       30908|               2.6|   MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy|      41.0|          44129|            49500|     93629|        4147|       32935|              2.39|   MA|               White| 58723|\n",
      "|          Hoover|      38.5|          38040|            46799|     84839|        4819|        8229|              2.58|   AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|      34.5|          88127|            87105|    175232|        5821|       33878|              3.18|   CA|Black or African-...| 24437|\n",
      "|          Newark|      34.6|         138040|           143873|    281913|        5829|       86253|              2.73|   NJ|               White| 76402|\n",
      "|          Peoria|      33.1|          56229|            62432|    118661|        6634|        7517|               2.4|   IL|American Indian a...|  1343|\n",
      "|        Avondale|      29.1|          38712|            41971|     80683|        4815|        8355|              3.18|   AZ|Black or African-...| 11592|\n",
      "|     West Covina|      39.8|          51629|            56860|    108489|        3800|       37038|              3.56|   CA|               Asian| 32716|\n",
      "|        O'Fallon|      36.0|          41762|            43270|     85032|        5783|        3269|              2.77|   MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|      35.5|          51751|            58077|    109828|        5204|       16315|              2.65|   NC|               Asian| 11060|\n",
      "|          Folsom|      40.9|          41051|            35317|     76368|        4187|       13234|              2.62|   CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|      40.9|          41051|            35317|     76368|        4187|       13234|              2.62|   CA|American Indian a...|   998|\n",
      "|    Philadelphia|      34.1|         741270|           826172|   1567442|       61995|      205339|              2.61|   PA|               Asian|122721|\n",
      "|         Wichita|      34.6|         192354|           197601|    389955|       23978|       40270|              2.56|   KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|      34.6|         192354|           197601|    389955|       23978|       40270|              2.56|   KS|American Indian a...|  8791|\n",
      "|      Fort Myers|      37.3|          36850|            37165|     74015|        4312|       15365|              2.45|   FL|               White| 50169|\n",
      "|      Pittsburgh|      32.9|         149690|           154695|    304385|       17728|       28187|              2.13|   PA|               White|208863|\n",
      "|          Laredo|      28.8|         124305|           131484|    255789|        4921|       68427|              3.66|   TX|American Indian a...|  1253|\n",
      "|        Berkeley|      32.5|          60142|            60829|    120971|        3736|       25000|              2.35|   CA|               Asian| 27089|\n",
      "|     Santa Clara|      35.2|          63278|            62938|    126216|        4426|       52281|              2.75|   CA|               White| 55847|\n",
      "+----------------+----------+---------------+-----------------+----------+------------+------------+------------------+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save data in parquet format on S3\n",
    "us_cities_demo.write.mode('overwrite').parquet(output_data + \"demographics/\")\n",
    "# load data from S3 \n",
    "df_demo = spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/demographics/\")\n",
    "# display data\n",
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's create a function for mapping\n",
    "def mapping_function(file_name, column_name1, column_name2):\n",
    "    file = open(file_name, 'r')\n",
    "    code = []\n",
    "    name = []\n",
    "    for i in file:\n",
    "        row = \" \".join(i.split())\n",
    "        code.append(row[:row.index('=')-1])\n",
    "        name.append(row[row.index('=')+1:])\n",
    "    file.close()\n",
    "    df = pd.DataFrame(list(zip(code,name)), columns = [f'{column_name1}', f'{column_name2}'])\n",
    "    df = spark.createDataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|             airport|\n",
      "+----+--------------------+\n",
      "| ALC|               alcan|\n",
      "| ANC|           anchorage|\n",
      "| BAR|baker aaf - baker...|\n",
      "| DAC|       daltons cache|\n",
      "| PIZ|dew station pt la...|\n",
      "| DTH|        dutch harbor|\n",
      "| EGL|               eagle|\n",
      "| FRB|           fairbanks|\n",
      "| HOM|               homer|\n",
      "| HYD|               hyder|\n",
      "| JUN|              juneau|\n",
      "| 5KE|           ketchikan|\n",
      "| KET|           ketchikan|\n",
      "| MOS|moses point inter...|\n",
      "| NIK|             nikiski|\n",
      "| NOM|                 nom|\n",
      "| PKC|         poker creek|\n",
      "| ORI|      port lions spb|\n",
      "| SKA|             skagway|\n",
      "| SNP|     st. paul island|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mapping data\n",
    "# load airport_code data for mapping\n",
    "airport_code = open('airports_code.txt', 'r')\n",
    "code = []\n",
    "airport = []\n",
    "for i in airport_code:\n",
    "        row = \" \".join(i.split())\n",
    "        code.append(row[:row.index('=')-1])\n",
    "        row = row.split(\",\")\n",
    "        row = row[0].lower()\n",
    "        row = row[row.index('=')+1:]\n",
    "        airport.append(row)\n",
    "airport_code.close()\n",
    "df = pd.DataFrame(list(zip(code,airport)),columns=['code', 'airport'])\n",
    "df = spark.createDataFrame(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's create a function to map airports\n",
    "def mapping_function_airport(file_name, column_name1, column_name2):\n",
    "    file = open(file_name, 'r')\n",
    "    code = []\n",
    "    name = []\n",
    "    for i in file:    \n",
    "        row = \" \".join(i.split())\n",
    "        code.append(row[:row.index('=')-1])\n",
    "        row = row.split(\",\")\n",
    "        row = row[0].lower()\n",
    "        row = row[row.index('=')+1:]\n",
    "        name.append(row)\n",
    "    file.close()\n",
    "    df = pd.DataFrame(list(zip(code,name)), columns = [f'{column_name1}', f'{column_name2}'])\n",
    "    df = spark.createDataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save data to S3\n",
    "output_data = \"s3a://udacity-data-engineering-stan/data/\"\n",
    "states.write.mode('overwrite').parquet(output_data + \"states_code/\")\n",
    "countries.write.mode('overwrite').parquet(output_data + \"countries_code/\")\n",
    "modes.write.mode('overwrite').parquet(output_data + \"modes_code/\")\n",
    "visas.write.mode('overwrite').parquet(output_data + \"visas_code/\")\n",
    "airports.write.mode('overwrite').parquet(output_data + \"airports_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create mapping dataframes\n",
    "states = mapping_function('states_code.txt', 'code', 'state')\n",
    "countries = mapping_function('countries_code.txt', 'code', 'country')\n",
    "modes = mapping_function('modes_code.txt', 'code', 'mode')\n",
    "visas = mapping_function('visas_code.txt', 'code', 'visa')\n",
    "airports = mapping_function_airport('airports_code.txt', 'code', 'airport')\n",
    "visas = visas.withColumn(\"code\", visas[\"code\"].cast(IntegerType()))\n",
    "modes = modes.withColumn(\"code\", modes[\"code\"].cast(IntegerType()))\n",
    "countries = countries.withColumn(\"code\", countries[\"code\"].cast(IntegerType()))\n",
    "countries = countries.where(col(\"code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load data from S3\n",
    "df_states = spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/states_code/\")\n",
    "df_countries = spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/countries_code/\")\n",
    "df_modes = spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/modes_code/\")\n",
    "df_visas = spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/visas_code/\")\n",
    "df_airports = spark.read.parquet(\"s3a://udacity-data-engineering-stan/data/airports_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import datediff, to_date, date_format\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS CREDS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS CREDS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    '''\n",
    "    This function creates a spark session\n",
    "    '''\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.1.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "    .config(\"spark.driver.memory\", \"15g\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    # this code speeds up parquet write\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")    \n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "    sc._jsc.hadoopConfiguration().set(\"spark.speculation\",\"false\")\n",
    "    print(spark)\n",
    "    return spark\n",
    "\n",
    "def i94_data(month, year):\n",
    "    '''\n",
    "    This function \n",
    "    '''\n",
    "    df_spark =spark.read.format('com.github.saurfang.sas.spark').load(f'../../data/18-83510-I94-Data-2016/i94_{month}{year}_sub.sas7bdat').drop_duplicates()\n",
    "    df_spark = df_spark.where(col(\"dtadfile\").isNotNull())\n",
    "    df_spark = df_spark.withColumn('stayed_days', ( df_spark['depdate'] - df_spark['arrdate']))\n",
    "    df_spark = df_spark.withColumn(\"allowed_stay_till\",  to_date(\"dtaddto\",\"MMddyyyy\"))\n",
    "    df_spark = df_spark.withColumn('date_created', to_date(\"dtadfile\", 'yyyyMMdd'))\n",
    "    df_spark = df_spark.withColumn(\"allowed_stay_days\", datediff('allowed_stay_till', 'date_created'))\n",
    "    df_spark.createOrReplaceTempView('i94')\n",
    "    query = \"\"\"SELECT cicid, \n",
    "                date_created, \n",
    "                i94yr AS year, \n",
    "                i94mon AS month,\n",
    "                i94cit AS citizenship,\n",
    "                i94res AS resident,\n",
    "                i94bir AS age,\n",
    "                biryear AS birth_year,\n",
    "                gender,\n",
    "                occup AS occupation,\n",
    "                allowed_stay_till,\n",
    "                allowed_stay_days,\n",
    "                stayed_days,\n",
    "                i94visa AS visa_class,\n",
    "                visatype AS visa_type,\n",
    "                i94port AS port,\n",
    "                i94mode AS mode,\n",
    "                i94addr AS arraval_state,\n",
    "                visapost AS visa_issued_by,\n",
    "                entdepa AS arrival_flag,\n",
    "                entdepd AS depart_flag,\n",
    "                entdepu AS update_flag,\n",
    "                matflag AS match_flag,\n",
    "                insnum AS ins_number,\n",
    "                airline,\n",
    "                admnum AS admission_number,\n",
    "                fltno AS flight_number\n",
    "           FROM i94\n",
    "       \"\"\"\n",
    "    i94 = spark.sql(query)\n",
    "    i94 = i94.withColumn(\"cicid\", i94[\"cicid\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"year\", i94[\"year\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"month\", i94[\"month\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"citizenship\", i94[\"citizenship\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"resident\", i94[\"resident\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"age\", i94[\"age\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"birth_year\", i94[\"birth_year\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"stayed_days\", i94[\"stayed_days\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"visa_class\", i94[\"visa_class\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"mode\", i94[\"mode\"].cast(IntegerType()))\n",
    "    i94 = i94.withColumn(\"admission_number\", i94[\"admission_number\"].cast(IntegerType()))\n",
    "    i94.write.mode('overwrite').partitionBy(\"month\", \"year\").parquet(output_data + \"immigration/\")\n",
    "    print('Immigration data was saved in parquet format on S3')\n",
    "    \n",
    "\n",
    "def airport_data():\n",
    "    fname = 'airport-codes_csv.csv'\n",
    "    airport_df = spark.read.csv(fname, header=True, inferSchema=True).drop_duplicates()\n",
    "    airport_df.createOrReplaceTempView('airports')\n",
    "    query = \"\"\"SELECT ident, \n",
    "                      type, \n",
    "                      name, \n",
    "                      elevation_ft, \n",
    "                      iso_country, \n",
    "                      iso_region, \n",
    "                      municipality, \n",
    "                      gps_code, \n",
    "                      iata_code AS airport_code, \n",
    "                      coordinates\n",
    "                 FROM airports \n",
    "                 WHERE iata_code IS NOT NULL \n",
    "                 AND NOT iata_code = 'nan'\n",
    "                 AND iso_country = 'US'\n",
    "            \"\"\"\n",
    "    airports = spark.sql(query)\n",
    "    \n",
    "    def state(iso_region):\n",
    "        return iso_region.strip().split('-')[-1]\n",
    "    udf_state = udf(lambda iso_region: state(iso_region), StringType())\n",
    "    airports = airports.withColumn('state', udf_state('iso_region')).drop('iso_region')\n",
    "    airports.write.mode('overwrite').parquet(output_data + \"airports_data/\")\n",
    "    print('Airport data was saved in parquet format on S3')\n",
    "    \n",
    "    \n",
    "def us_demo_data():\n",
    "    fname = 'us-cities-demographics.csv'\n",
    "    demo_df = pd.read_csv(fname, ';')\n",
    "    demo_df = spark.createDataFrame(demo_df)\n",
    "    demo_df.createOrReplaceTempView('demographics')\n",
    "    query = \"\"\"SELECT city, \n",
    "                     `Median Age` AS median_age, \n",
    "                     `Male Population` AS male_population,\n",
    "                     `Female Population` AS female_population, \n",
    "                     `Total Population` AS population,\n",
    "                     `Number of Veterans` AS num_veterans, \n",
    "                     `Foreign-born` AS foreign_born, \n",
    "                     `Average Household Size` AS avg_household_size,\n",
    "                     `State Code` AS state, \n",
    "                     race, \n",
    "                     count\n",
    "                FROM demographics\"\"\"\n",
    "    us_cities_demo = spark.sql(query)\n",
    "    us_cities_demo = us_cities_demo.withColumn(\"male_population\", us_cities_demo[\"male_population\"].cast(IntegerType()))\n",
    "    us_cities_demo = us_cities_demo.withColumn(\"female_population\", us_cities_demo[\"female_population\"].cast(IntegerType()))\n",
    "    us_cities_demo = us_cities_demo.withColumn(\"num_veterans\", us_cities_demo[\"num_veterans\"].cast(IntegerType()))\n",
    "    us_cities_demo = us_cities_demo.withColumn(\"foreign_born\", us_cities_demo[\"foreign_born\"].cast(IntegerType()))\n",
    "    us_cities_demo.write.mode('overwrite').parquet(output_data + \"demographics/\")\n",
    "    print('US cities data was saved in parquet format on S3')\n",
    "    \n",
    "\n",
    "def temp_data():\n",
    "    fname = '/data2/GlobalLandTemperaturesByCity.csv'\n",
    "    temp_df = spark.read.csv(fname, header=True)\n",
    "    temp_df = temp_df.withColumn('month', month(\"dt\"))\\\n",
    "            .withColumn('year', year('dt'))\\\n",
    "            .withColumn('day', dayofmonth('dt'))\\\n",
    "            .withColumn('id', monotonically_increasing_id())\n",
    "    temp_df.createOrReplaceTempView('temperatures')\n",
    "    query = \"\"\"SELECT id,\n",
    "                      AverageTemperature AS avg_temp,\n",
    "                      AverageTemperatureUncertainty AS sd_temp,\n",
    "                      LOWER(City) AS city,\n",
    "                      Country AS country,\n",
    "                      month,\n",
    "                      day\n",
    "                 FROM temperatures\n",
    "                 WHERE Country = 'United States'\n",
    "                 AND year = 2012\n",
    "            \"\"\"\n",
    "    temperatures = spark.sql(query)\n",
    "    temperatures = temperatures.withColumn(\"avg_temp\", temperatures[\"avg_temp\"].cast(FloatType()))\n",
    "    temperatures = temperatures.withColumn(\"sd_temp\", temperatures[\"sd_temp\"].cast(FloatType()))\n",
    "    temperatures = temperatures.withColumn(\"id\", temperatures[\"id\"].cast(IntegerType()))\n",
    "    temperatures.write.parquet(output_data + 'temperatures/', 'overwrite')\n",
    "    print('Temperature data was saved in parquet format on S3')\n",
    "\n",
    "\n",
    "def mapping_function(file_name):\n",
    "    file = open(file_name+'_code.txt', 'r')\n",
    "    code = []\n",
    "    name = []\n",
    "    for i in file:\n",
    "        row = \" \".join(i.split())\n",
    "        code.append(row[:row.index('=')-1])\n",
    "        name.append(row[row.index('=')+1:])\n",
    "    file.close()\n",
    "    df = pd.DataFrame(list(zip(code,name)), columns = ['code', 'name'])\n",
    "    df = spark.createDataFrame(df)\n",
    "    if file_name == 'visas':\n",
    "        df = df.withColumn(\"code\", df[\"code\"].cast(IntegerType()))\n",
    "    if file_name == 'modes': \n",
    "        df = df.withColumn(\"code\", df[\"code\"].cast(IntegerType()))\n",
    "    if file_name == 'countries':  \n",
    "        df = df.withColumn(\"code\", df[\"code\"].cast(IntegerType()))\n",
    "        df = df.where(col(\"code\").isNotNull())\n",
    "    df.write.mode('overwrite').parquet(output_data + f\"{file_name} + '_code'/\")\n",
    "    print(file_name + ' data was saved in parquet format on S3')\n",
    "\n",
    "def mapping_function_airport(file_name):\n",
    "    file = open(file_name+'_code.txt', 'r')\n",
    "    code = []\n",
    "    name = []\n",
    "    for i in file:    \n",
    "        row = \" \".join(i.split())\n",
    "        code.append(row[:row.index('=')-1])\n",
    "        row = row.split(\",\")\n",
    "        row = row[0].lower()\n",
    "        row = row[row.index('=')+1:]\n",
    "        name.append(row)\n",
    "    file.close()\n",
    "    df = pd.DataFrame(list(zip(code,name)), columns = ['code', 'name'])\n",
    "    df = spark.createDataFrame(df)\n",
    "    df.write.mode('overwrite').parquet(output_data + f\"{file_name} +'_code'/\")\n",
    "    print(file_name + ' data was saved in parquet format on S3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries data was saved in parquet format on S3\n",
      "states data was saved in parquet format on S3\n",
      "visas data was saved in parquet format on S3\n",
      "modes data was saved in parquet format on S3\n"
     ]
    }
   ],
   "source": [
    "output_data = \"s3a://udacity-data-engineering-stan/data/\"\n",
    "spark=create_spark_session()\n",
    "i94_data('jan', 16)\n",
    "airport_data()\n",
    "us_demo_data()\n",
    "mapping_list=['countries','states','visas','modes']\n",
    "for i in mapping_list:\n",
    "    mapping_function(i)\n",
    "mapping_function_airport('airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Data quality checks were run by the Airflow pipeline during the process of uploading data to the Redshift database.\n",
    "\n",
    "<img src=\"airflow1.jpg\">\n",
    "\n",
    "<img src=\"airflow2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "- Spark, S3, Airflow and Redshift have been used for this project. Spark allows fast in-memory parallel data processing. Spark was used to create columnar format files that have been loaded to S3. S3 is a key-value store that can be represented as a data lake. Airflow was used to create a pipeline to create a database and all tables in Redshift and load the data to the tables. \n",
    "* Propose how often the data should be updated and why.\n",
    "- Certain tables that are related to i94 data and temperature should be updated daily via Airflow scheduler as this data gets generated on daily basis and should be fresh to provide the most accurate analytical reports and dashboards.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " - The current project was designed with big data in mind, both Spark, Airflow and Redshift allow fast scalability for growing data and user throughput. \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7 am every day.\n",
    " - Airflow can be used to schedule data ingestion to the Redshift data warehouse.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " - Many cloud providers offer instant scalability, for example, Amazon EMR servers can be upscaled to provide more power to data transformation and Redshift can be auto-scaled to provide smooth access to the data to all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Sources:  https://github.com/srkucd/data_engineering_capstone/blob/master/prototype.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
