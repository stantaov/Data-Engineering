# Project: Data Pipelines with Airflow

This project was offered as part of Udacity's Data Engineering Nanodegree program.

## Introduction

A fictional music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.

They have decided to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

## Project Description

To complete the project, I created custom operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step.

Helpers class was created and contains all the SQL transformations. Thus, all ETL process were ran via custom operators.

## Example of Airflow Pipeline

![image](example-dag.png)

## Project Datasets

I was given two datasets that reside in S3. Here are the S3 links for each:

- Song data: s3://udacity-dend/song_data
- Log data: s3://udacity-dend/log_data

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}```

### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

## Configuring the main DAG

The main DAG is scheduled to run once an hour.
In the main DAG, I added default parameters according to the guidelines:

The DAG does not have dependencies on past runs
On failure, the task are retried 3 times
Retries happen every 5 minutes
Catchup is turned off
Do not email on retry


## Operators

To complete the project, I needed to build four different operators that will stage the data, transform the data, and run checks on data quality.

All of the operators and task instances run SQL statements against the database on Redshift cluster.

### Stage Operator

The stage operator loads all avalible JSON formatted files from S3 to Amazon Redshift. The operator creates and runs a SQL COPY statement based on the parameters provided. The operator's parameters specifies where in S3 the file is loaded and what is the target table.

The parameters distinguish between JSON files. The stage operator loads timestamped files from S3 based on the execution time and run backfills.

### Fact and Dimension Operators

Dimension and fact operators utilize the SQL helper class to run data transformations. These operators take as input a SQL statement and target database on which to run the query against.

Dimension and Fact loads allow to switch between append-only and delete-load functionality. Dimension loads are often done with the truncate-insert pattern where the target table is emptied before the load. Fact tables are massive so I used append-only type functionality.

### Data Quality Operator

The data quality operator is used to run a simple check on the data itself. The operator should raises an exception and the task fail eventually.

## How to Run the Project

1. Create a Redshift cluster, it should located in the same region as S3 blucket. Make sure it publicly avalible and your IP is whitelisted on security groups.

2. Install Airflow. There is a details guide on how to instal it with docker (https://towardsdatascience.com/getting-started-with-airflow-using-docker-cd8b44dbff98)
 
3. Config Airflow. Create AWS connection with your AWS credentials. Next, create another connection for Redshift cluster using Postgress connector. 

3. Run the tables DAG in your Airflow interface to create all necessary tables on Redshift cluster. 

4. Next, enable the main DAG to start the data process ingestion into the Redshift cluster tables and run data quality checks.
